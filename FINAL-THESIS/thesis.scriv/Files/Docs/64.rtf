{\rtf1\ansi\ansicpg1252\cocoartf1347\cocoasubrtf570
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red255\green0\blue0;}
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\f0\b\fs36 \cf0 \\section\{Automatisk orddeling\}\
\\label\{sec:automatisk-orddeling\}\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\b0\fs24 \cf0 Mye arbeidet har g\'e5tt med til \'e5 skape algoritmer for automatisk orddeling ved hjelp av datamaskiner. Disse metodene kan generelt sett bli delt inn i tre kategorier for tiln\'e6rminger; regelbaserte, ordlistebaserte og m\'f8nsterbaserte algoritmer. Ofte ser vi at algoritmer bruker en kombinasjon av disse for \'e5 kunne oppn\'e5 best mulig resultat. I dette kapittelet vil jeg rask g\'e5 gjennom disse metodene, gi noen eksempler p\'e5 deres bruk og raskt diskutere deres styrker og svakheter.
\b \
\
\\subsection\{Ordlistebaserte metoder\}\
\

\b0 Den ordlistebaserte algoritmen er kanskje den mest innlysende fremgangsm\'e5ten. Den best\'e5r simpelthen av en ordliste som viser ord med alle lovlige delepunkter. For \'e5 dele et ord trenger algoritmen kun \'e5 s\'f8ke etter ordet som skal deles i en datastruktur. \
\
Nyhetsavisen Aftenposten benyttet seg av en slik tiln\'e6rming p\'e5 90-tallet. IBM sto bak arbeidet og utviklet en algoritme som benyttet seg av en liste av over 1,2 millioner delte ord. Hvis ordet som skulle deles ikke ble funnet i ordlisten, ble det fors\'f8kt delt etter en form for regelbasert logikk. En fulltidsansatt orddeler ville s\'e5 g\'e5 over ordet og kontrollere det. Hvis de s\'e5 at ett enkelt ord var \'f8nsket \'e5 deles mer en et gitt antall ganger ville det s\'e5 bli lagt inn tilbake i ordlisten igjen, med tanken om at da var ordet s\'e5pass mye brukt at det h\'f8yst sannsynlig ville dukke opp flere ganger. Siden den gang har Aftenposten g\'e5tt bort fra denne l\'f8sningen og bruker et propriet\'e6rt system. [Mail, Gunnar]\
\
Et problem med slike lister er at de blir enormt store. I tillegg til at alle ord m\'e5 lagres m\'e5 ogs\'e5 alle varianter av et ord lagres; for eksempel kake, kaker, kakene. Som svar p\'e5 dette ble andre metoder utviklet -- som Time Magazine-algoritmen. Den s\'e5 p\'e5 alle firebokstavskombinasjoner rundt hvert delepunkt og beregenet sannsynligheten for at en delepunkt kunne oppst\'e5 p\'e5 denne plasseringen. For \'e5 spare plass, som var viktig p\'e5 denne tiden, gjorde man det at i stedenfor \'e5 lagre alle firebokstavkombinasjoner, som ga $26^4$ kombinasjoner, s\'e5 de heller p\'e5 par av to og to bokstaver og lagret de i tre forskjellige tabeller. For eksempel ordet \'abep-le\'bb ville blitt lagret i tre forskjellige tabeller av st\'f8rrelse $26^2$, som \'abep\'bb, \'abpl\'bb og \'able\'bb. Denne sannsynligheten ble s\'e5 sammenlignet med en grenseverdi som sa om det var riktig med et delepunkt her eller ikke. \
\
En komplett ordliste ville gitt den perfekte l\'f8sningen p\'e5 orddelingsproblemet (med unntak av homografer). Den ville funnet 100 \\% av alle delepunkter med 0 \\% feil. Men dette er selvf\'f8lgelig utopisk og umulig. Regler for orddeling endrer seg, nye ord kommer til og spr\'e5k med stor bruk av sammensatte ord, som norsk, har muligheten til \'e5 generere nye ord som ikke har v\'e6rt sett tidligere ved \'e5 kombinere gamle eksisterende ord. Vi ser det nesten hver dag i avisen, som for eksempel ordet \'abskuldersurfing\'bb som jeg kom over dagen jeg skrev dette avsnittet \\cite\{skuldersurfing\}. S\'e5 en ordlistebasert tiln\'e6rming vil aldri kunne dele alle ord, den m\'e5 kombineres med en av de andre metodene (slik Aftenposten gjorde) eller i kombinasjon med manuelt arbeid.
\b\fs36 \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\fs24 \cf0 \\subsection\{Regelbaserte metoder\}\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\b0 \cf0 De regelbaserte orddelingsalgoritmene tar grunnlag i de offisielle orddelingsreglene for et gitt spr\'e5k, og pr\'f8ver s\'e5 godt det lar seg gj\'f8re \'e5 klassifisere ord inn i grupper og p\'e5f\'f8re de gjeldene reglene.\
\
Los Angeles Times utviklet en regelbasert orddelingsalgoritme som skulle brukes i deres typsettingssystem. Algoritmen registrerte og klassifiserte ord inn i vokal- og konsonantm\'f8nstere, som igjen ble delt inn i fire forskjellige grupper hvor de gjeldene reglene ble p\'e5f\'f8rt. I tilegg la de til regler for spesielle tilfeller og regler for h\'e5ndtering av prefiks- og suffiksregler. Denne tiln\'e6rmingen ble beskrevet til \'e5 v\'e6re 85--95 prosent n\'f8yaktig. [Liang Thesis] De nevner her ikke hvor mange feiltreff denne algoritmen generelt sett har. \
\
En slik tiln\'e6rming har flere svakheter. F\'f8rst av alt er det en sv\'e6rt lite generell tiln\'e6rming til problemet. Forskjellige spr\'e5k kan ha sv\'e6rt forskjellige regler for orddeling, og algoritmene m\'e5 enten drastisk tilpasses eller skrives helt p\'e5 nytt for \'e5 kunne benyttes med et annet spr\'e5k. En slik tiln\'e6rming er ogs\'e5 problematisk for spr\'e5k hvor delepunktet er basert p\'e5 uttale av ordet, slik som amerikansk-engelsk. Uttale kan raskt endre seg over tid, og da er man n\'f8dt til \'e5 endre selve algoritmen for \'e5 omfavne disse nye endringene. Det holder ikke \'e5 endre noen parametere eller kj\'f8re programmet med ny inputdata for \'e5 tilpasse resultatet, slik det kan v\'e6re mulig med de andre tiln\'e6rmingene. Et annet problem er at det alltids finnes unntak fra regler og en liste som utfyllende skal beskrive alle unntak kan bli un\'f8dvendig stor og vanskelig \'e5 h\'e5ndtere. Til sist, sammensatte ord, som vi har mye av i det norske spr\'e5ket, kan v\'e6re vanskelig for en datamaskin \'e5 analysere korrekt. Hvordan skal en datamaskin kunne forst\'e5 at \'abfylkestrafikksikkerhetsutvalgssekritariatslederfunksjonene\'bb er en sammensatt kombinasjon av mange ord og at den m\'e5 finne alle disse sammensettningene for \'e5 finne riktig delepunkt? For ikke \'e5 nevne at den ogs\'e5 m\'e5 gjennkjenne komposisjonsfuger i ordet for \'e5 finne alle riktige delepunkt. [Liang og Gunnar]\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\b \cf0 \
\\subsection\{M\'f8nsterbaserte metoder\}\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\b0 \cf0 En m\'f8nsterbasert tiln\'e6rming vil enkelt forklart lagre en serie av bokstaver (m\'f8nstere) som vil spesifisere hvor det er mulighet for \'e5 sette inn et delepunkt. N\'e5r man vil dele et ord ser man om et av de lagrede m\'f8nsterene matcher en del av ordet, og vi vil da kunne se hvor dette ordet kan deles. For eksempel kunne vi ha lagret m\'f8nsteret \'absjon.\'bb som forteller oss at det er lov \'e5 sette inn delepunkt f\'f8r ordet, alts\'e5 \'ab-sjon\'bb. Et slikt m\'f8nster vil s\'e5 for eksempel kunne gjelde for ordene \'absta-sjon\'bb, \'abna-sjon\'bb og \'abak-sjon\'bb. Ofte bruker man punktumtegnet \'ab.\'bb for \'e5 merke start eller slutt p\'e5 et ord i m\'f8nsteret. Tiln\'e6rminger som dette har en tendens til \'e5 kunne bli ganske kompakte, is\'e6r i sammenligning med ordlistebaserte metoder, som m\'e5 lagre alle varianter av et ord. \
\
Som med alt annet er det ogs\'e5 unntak til slike regler, de vil ikke kunne gi riktig delepunkt i alle tilfeller. Liang og \\TeX\{\}-algoritmen l\'f8ste dette ved \'e5 introdusere niv\'e5er der et m\'f8nster som har et tall p\'e5 et h\'f8yere niv\'e5 vil v\'e6re et \'abunntak til unntaket\'bb, der det h\'f8yeste tallet bestemmer. Et oddetall betyr at det er lovlig med delepunkt, mens partall forteller at det ikke er lov med delepunkt. [Liang]\
\
Et problem med slike algoritmer er at de krever tilgang p\'e5 en tilstrekkelig stor ordliste for et gitt spr\'e5k som viser alle lovlige delepunkter i ordene, for \'e5 kunne generere gode nok m\'f8nstere. For enkelte spr\'e5k, slik som amerikansk-engelsk er slike lister tilgjengelig. Men for norsk finnes ingen slike lister, og m\'f8nsterene som er generert til norsk \\TeX\{\} er basert p\'e5 en relativt liten liste av ord som er delt for h\'e5nd. [Gunnar]\
\
\cf2 Skriv ogs\'e5 om nye m\'f8nstere.\cf0 \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\b\fs36 \cf0 \\input\{content/nevrale_nettverk.tex\}}