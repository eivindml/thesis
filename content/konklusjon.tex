\chapter{Konklusjon}
\label{sec:konklusjon}

Det første å legge merke til er hvor komplekst, og interessant, det tilsynelatende enkle problemet om automatisk orddeling faktisk er. For et menneske oppfattes det intuitivt, og det er enkelt å dele et ord. Når en skal forsøke å fortelle en datamaskin hvordan den kan gjøre det på egenhånd, blir kompleksiteten en helt annen. I denne oppgaven ønsket vi å utvikle en regelbasert tilnærming for automatisk orddeling for å
\begin{inparaenum}[\itshape a\upshape)]
\item øke presisjonen på delepunkter som blir funnet, ved flere korrekte delepunkter og færre gale delepunkter, og 
\item bevare informasjon om hvilke orddelingsregler som ga opphav til delepunkene, for å gi systemene for tekstsetting økt kontroll.
\end{inparaenum}
Vedrørende punkt a.) har vi ikke lykkes. Vi finner færre korrekte delepunkter og en uakseptabel mengde gale delepunkter. Når det gjelder punkt b.), har dette vist seg å være fullt mulig. Vi kan spesifisere nøyaktig hvilke av orddelingsreglene som skal benyttes for å dele et ord.

Resultatet fra arbeidet med denne oppgaven kan oppsummeres i følgende punkter:
\begin{enum}
\item Vi har gitt et overblikk over problemstillingen rundt automatisk orddeling og hvilke metoder som eksisterer, med et hovedfokus på metoder tilgjengelig for norsk. 
\item Vi har presentert detaljer for hvordan en kan implementere en modulær, regelbasert algoritme for orddeling. 
\item Vi har bidratt med kunnskap om frekvensen av forekomster av de forskjellige fugebokstavene i sammensatte ord.
\end{enum}

Basert på arbeidet med denne oppgaven vil jeg presentere de viktigste erfaringene å ta med seg videre. Til slutt, for at en regelbasert tilnærming til automatisk orddeling skal bli nyttig, trengs det en rekke forbedringer. I siste kapittel diskuterer jeg mulighetene for videre arbeid.

\section{Hva har vi lært?}

Først og fremst: automatisk orddeling er komplisert! Dette var et mer omfattende problem enn det jeg forestilte meg da jeg gikk i gang med arbeidet. Underveis har jeg møtt på mange utfordringer som kunne vært egnet som egne studier i seg selv. Blant annet: dekomponering av sammensatte ord til \textit{alle} rotordbestanddelene, tolkning av sammensatte ord, tolkning av bindebokstaver, håndtering av trippelskrevet konsonant, telling og markering av stavelser i ord, markering av affikser i ord og effektiv innlesing, og søk i store ordlister. Denne oppgaven bør sees på som en skisse for en mulig vei videre for å utvikle en fullverdig løsning og for videre testing.

For det andre: Reglene for hvordan ord skal deles er noen ganger vanskelig å tolke. Slik jeg tolker enkonsonantregelen\sidenote{Enkonsonantregelen: I ord med en eller flere konsonanter mellom vokaler går det én konsonant til ny linje \cite{vinje}.} vil o\textbf{-}ran-gu-tang være en lovlig deling. Altså at vi får én vokal alene på første linje. Men Vinje deler ordet som oran-gu-tang i sin bok. Derimot viser han eksempelet a-le-ne som korrekt, hvor dette er tilfelle. Listen over delte ord fra Lars Gunnar Thoresen \cite{thoresen1993virtuelle}, som benyttes til testing av modulene, er heller ikke konsekvent med dette. Derfor ville det vært svært ønskelig med en offisiell liste som viser korrekte delinger for å kunne utføre mer nøyaktige tester.

Til sist: det er fullt mulig å utvikle et godt fungerende program for orddeling som gir økt kontroll over hvilke orddelingsregler som skal benyttes. Men for at dette programmet skal bli nyttig i bruk, trengs det ytterligere arbeid for å øke presisjonen.


\section{Videre arbeid}
\label{sec:videre-arbeid}

For at den regelbaserte algoritmen for automatisk orddeling skal bli nyttig, trengs det mer arbeid. Systemet har en svært modulær arkitektur og er tilgjengelig med en fri lisens, som gjør det enkelt å studere og forbedre deler av systemet individuelt. Det er flere interessante spørsmål som trenger svar, og deler som trenger videre arbeid. Dette er de viktigste:

\begin{items}
\item Av resultatene ser vi at CompoundInterpreter er modulen som gir flest feil. Før å øke presisjonen til programmet vil det ha størst effekt å forbedre denne modulen. Johannesen og Hauglin \cite{johannessen1996automatic} hevder at deres program basert på de samme reglene ga feil analyse i kun 1,1 \% av tilfellene. Det tyder på at det her er stort forbedringspotensiale for modulen. I kapittel \ref{sec:fuge-bokstav} presenteres også en del karakteristikker for binde-s og binde-e, som kan være interessant å se på muligheten for å implementere i modulen.
\item For CompoundInterpreter kan det også være interessant å se på muligheten for å tolke sammensatte ord hvor deler av ordet ikke er kjent for ordboken. Johannesen og Hauglin \cite[s. 10]{johannessen1996automatic} beskriver en teknikk for dette.
\item Slik programmet er nå klarer den ikke å skille mellom sammensatte ord og ikke-sammensatte ord når de blir gitt til modulen CompoundSplitter. Norsk ordbank inneholder både sammensatte og ikke-sammensatte ord, slik at det ikke bare er å gjøre et oppslag der for å teste dette. Det fører til at selv korte ikke-sammensatte ord vil bli dekomponert til rotordene, som for eksempel «sele», «se» og «le». Bokmålsordboka på nett \sidenote{\url{http://www.nob-ordbok.uio.no/perl/ordbok.cgi?OPP=bindestrek&bokmaal=+&ordbok=begge}} har nylig innført markering av hovedfuge i sammensatte ord, eksempelvis «binde|strek». Hvis disse endringene etterhvert reflekteres i den tilgjengelige utgaven av Norsk ordbank, som ligger til grunne for Bokmålsordboka på nett, vil det da være mulig å skille mellom sammensatte og ikke-sammensatte ord ved et oppslag i ordboka.
\item Regelen om at enstavelsesord ikke skal deles krever en funksjon som kan telle antall stavelser i et ord. I det minste svare om ordet har én eller flere stavelser. I dagens løsning gjøres det noe naivt (se seksjon \ref{sec:dictionary}). Den vil eksempelvis feilaktig anta at ordet «reell» kun har én stavelse, når den har to. 
\item Knyttet til punktet over, har vi mangelen i programmet for å «dele mellom vokaler når de hører til hver sin stavelse», som er tillatt av enkonsonantregelen. 
\item I implementasjonen var vi nødt til å gå bort fra forsøk på å tolke dobbeltskrevet konsonant som ved deling blir trippelskrevet. For eksempel madrasselger, som delt blir til madrass-selger. Ved forsøk på å tolke slike tilfeller av ordsammensetninger endte vi med totalt flere feiltolkninger av sammensatte ord enn om vi unnlot dette. Det kunne vært interessant å sett på omfanget av slike tilfeller av sammensatte ord, og hvordan man eventuelt kan tolke dem korrekt.
\item Til sist, CompoundInterpreter vil forsøke å finne hovedfugen i et ord, altså «verdenscup-seiere» og ikke «[verdens+cup]+seiere». Det er tillatt å dele ord mellom alle fugene i sammensatte ord som består av tre eller fler rotord, selv om det er \textit{ønskelig} å dele i hovedfugen. Det ville vært interessant å se om dette lar seg gjøre. Men dette er et vanskelig problem å løse, man ender fort opp med en overdekomponering, eksempelvis «fagplanarbeid» som blir til «fag+plan+ar+be+id».
\end{items}


