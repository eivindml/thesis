{\rtf1\ansi\ansicpg1252\cocoartf1347\cocoasubrtf570
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red255\green0\blue0;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid102\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid103\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid104\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid105\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid202\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid203\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid204\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid205\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid301\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid302\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid303\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid304\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid305\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid401\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid402\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid403\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid404\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid405\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid5}
{\list\listtemplateid6\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid501\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid502\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid503\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid6}
{\list\listtemplateid7\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid601\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid602\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid603\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid7}
{\list\listtemplateid8\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid701\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid702\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid703\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid8}
{\list\listtemplateid9\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid801\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid802\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid803\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid9}
{\list\listtemplateid10\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid901\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid902\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid903\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid10}
{\list\listtemplateid11\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1001\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1002\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1003\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid11}
{\list\listtemplateid12\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1102\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1103\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid12}
{\list\listtemplateid13\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1202\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1203\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid13}
{\list\listtemplateid14\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1301\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1302\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1303\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid14}
{\list\listtemplateid15\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1401\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1402\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid15}
{\list\listtemplateid16\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1501\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1502\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid16}
{\list\listtemplateid17\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1601\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1602\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid17}
{\list\listtemplateid18\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1701\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1702\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid18}
{\list\listtemplateid19\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1801\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1802\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid19}
{\list\listtemplateid20\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1901\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1902\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid20}
{\list\listtemplateid21\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid2001\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2002\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid21}
{\list\listtemplateid22\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid2101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2102\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid22}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}{\listoverride\listid6\listoverridecount0\ls6}{\listoverride\listid7\listoverridecount0\ls7}{\listoverride\listid8\listoverridecount0\ls8}{\listoverride\listid9\listoverridecount0\ls9}{\listoverride\listid10\listoverridecount0\ls10}{\listoverride\listid11\listoverridecount0\ls11}{\listoverride\listid12\listoverridecount0\ls12}{\listoverride\listid13\listoverridecount0\ls13}{\listoverride\listid14\listoverridecount0\ls14}{\listoverride\listid15\listoverridecount0\ls15}{\listoverride\listid16\listoverridecount0\ls16}{\listoverride\listid17\listoverridecount0\ls17}{\listoverride\listid18\listoverridecount0\ls18}{\listoverride\listid19\listoverridecount0\ls19}{\listoverride\listid20\listoverridecount0\ls20}{\listoverride\listid21\listoverridecount0\ls21}{\listoverride\listid22\listoverridecount0\ls22}}
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\f0\fs24 \cf0 I denne delen av oppgaven \'f8nsker jeg \'e5 se p\'e5 mulighet for en ny tiln\'e6rming til problemet om automatisk orddeling. Metodene som eksisterer i dag er stort sett m\'f8nsterbaserte (\\TeX\{\}-algoritmen og de basert p\'e5 nevrale nettverk). Disse har en ganske god presisjon med henholdsvis X og Y prosent av korrekte delinger funnet og A og B prosent feil delinger (se kapittel \\ref\{sec:tid-arb-orddeling\}). Det er selvf\'f8lgelig \'f8nskelig at korrekte delepunkter funnet er st\'f8rre og spesielt at antall feil delepunkter ytterligere reduseres. En enda st\'f8rre mangel ved de eksisterende metodene for orddeling er mangelen for \'e5 kunne skille p\'e5 de forskjellige typer av orddeling vi har i norsk (se kapittel \\ref\{sec:orddelingsreglene\}). Eksempelvis sier Finn-Erik Vinjes Skriveregler at sammensatte ord \'ab\'85 deles prim\'e6rt mellom de ordene de best\'e5r av\'bb og at \'abi sammensetninger med mer enn to ord (ledd) b\'f8r man dele der hovedgrensen g\'e5r\'bb \\cite\{vinje\}. Dette er delinger de gjeldene metodene ikke har mulighet til \'e5 skille mellom. De vil generere alle mulige delepunkter (s\'e5 langt de klarer), men uten informasjon om hvilken regel som ga opphav til de forskjellige delepunktene. Derfor ser jeg det \'f8nskelig \'e5 finne en ny tiln\'e6rming til orddelingsproblemet som l\'f8ser dette ved \'e5 gi kontroll over hvilke regler som benyttes for orddelingen og \'f8nskelig gir en h\'f8yere presisjon i funn av delepunkter.\
\
I kapittel \\ref\{sec:automatisk-orddeling\} presenterte jeg de tre hovedkategoriene av metoder for automatisk orddeling: \\textit\{ordlistebaserte\}, \\textit\{m\'f8nsterbaserte\} og \\textit\{regelbaserte\}. Historisk har vi sett at ordlistebaserte tiln\'e6rminger har v\'e6rt brukt, eksempelvis Aftenposten sin liste over 1,2 millioner ferdigdelte ord, brukt i typsettingsystemet deres p\'e5 90-tallet. Denne metoden har \'e5penbare svakheter. Nyord som introduseres til spr\'e5ket m\'e5 kontinuerlig markers med delepunkter og legges til listen, og den vil ha problemer med den produktive ordsammensetningen vi har i norsk spr\'e5k. Former for m\'f8nsterbaserte algoritmer er det vi i all hovedsak ser er i bruk i dagens typsettingsystemer som \\TeX\{\}, OpenOffice og InDesign, alle basert p\'e5 Donal Knuth og Frank Liang sin algoritme \\cite\{smrvz1996word, knuth1986texbook, wiki-tex\}. Ogs\'e5 i mer akademisk sammenhenger er de fleste m\'f8nsterbaserte (nevrale nettverk) \\cite\{nemeth2006automatic, kristensen1998two\}. Av karakter vil de m\'f8nsterbaserte algoritmene gj\'f8re generaliseringer som f\'f8rer til feil delinger (jo mer presise m\'f8nsterene blir, jo mer nermer de seg en ordliste over delepunkter), og mer de klarer ikke skille mellom hvilken regel for orddeling som ga opphav til delepunktet. Den siste kategorien av orddelingsalgoritmer, regelbaserte, har jeg ikke sett v\'e6rt fors\'f8kt rettet mot norsk spr\'e5k med norske orddelingsregler. Slike tiln\'e6rminger har v\'e6rt kritisert, blant annet av X[Finn kilde], for \'e5 v\'e6re lite generelle \'97 de m\'e5 utvikles spesifikt rettet mot hver skriftspr\'e5k med sine unike regler. Dette ser jeg heller som en styrke. Ved en analytisk fremgangsm\'e5te har vi mulighet til \'e5 \'f8ke presisjonen rettet mot de gjeldene reglene for orddeling og vi kan bevare informasjonen om reglene som gir opphav til de forskjellige delepunktene, som kan gi bruker og applikasjoner kontroll til \'e5 selv bestemme hvilke regler som skal benyttes (for eksempel kun deling mellom hovedfugen i sammensatte ord, som kan sies \'e5 v\'e6re deling av h\'f8yere kvalitet en for eksempel deling med enkonsonantregelen). Jeg vil derfor benytte meg av sistnevnte tiln\'e6rming.\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\b\fs56 \cf0 \
\\section\{Applikasjonen\}\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\b0\fs24 \cf2 \
%Lag illustrasjon av informasjon som kommer med i delingen. F.eks. u-redd-kj\'e6rl-ig-het-en (prefiks, enkons, sammensatt, avledning, b\'f8yning)\cf0 \
\
Ved \'e5 se p\'e5 reglene for orddeling (kapittel \\ref\{sec:orddelingsregler\}) har jeg identifisert en hovedflyt for programmet (figur \\ref\{fig:app-flow\}). F\'f8r man kan p\'e5f\'f8re orddelingsreglene p\'e5 et ord (HyphenationRules), m\'e5 man f\'f8rst identifisere om ordet er sammensatt; hvis ikke \'97 p\'e5f\'f8r orddelingsreglene p\'e5 ordet. Om ordet er sammensatt, m\'e5 ordet deles i komponentene det er sammensatt av. Dette foreg\'e5r i to steg, l\'f8st basert p\'e5 metoden til Sj\'f6berg og Kann, beskrevet i kapittel \\ref\{sec:sammensatt-analyse\}. F\'f8rst blir ordet dekomponert i CompoundSplitter til alle lovlige dekomponeringer, ved hjelp av en ordliste. Deretter, om det finnes fler enn \'e9n dekomponeringsmuligheter blir det fors\'f8kt valgt ut den dekomponeringen som mest sannynlig er den korrekte eller \'f8nskede. Informasjon om binde-s og binde-e tas med i betraktning, via EphenthesisAnalyser, basert p\'e5 funnene til Johannesen og Hauglin, beskrevet i kapittel \\ref\{sec:reg-bind\}. Videre vil jeg gi en mer detaljert beskrivelse av hver modul (fremhevet i gr\'f8nt i figur \\ref\{fig:app-flow\}) i programflyten.\
\
\\begin\{figure\}[H]\
\\centering\
\\input\{content/figures/app-flow.tex\}\
\\label\{fig:app-flow\}\
\\caption\{Programflyt.\}\
\\end\{figure\}\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\b\fs36 \cf0 \\subsection\{Modulene\}
\fs24 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\b0 \cf0 \
\\subsubsection\{Dictionary\}\
\
Dictionary-modulen fungerer som et grensesnitt mot en ordbank. Alle modulene vil ha behov for \'e5 kunne gj\'f8re oppslag i en ordbok og hente ut relevant informasjon, b\'e5de oppslag om ord er representert i ordboken og om ordenes morfologiske beskrivelse. For ordliste som ligger til grunn for denne modulen vil jeg benytte Norsk ordbank, utviklet og vedlikeholdt av Institutt for lingvistiske og nordiske studier ved Universitetet i Oslo og Spr\'e5kr\'e5det, tilgjengelig som eksportert data i r\'e5tekstformat, fritt tilgjengelig med en GPL-lisens\\footnote\{{\field{\*\fldinst{HYPERLINK "http://www.edd.uio.no/prosjekt/ordbanken/"}}{\fldrslt http://www.edd.uio.no/prosjekt/ordbanken/}}\}. Dette er den mest omfattende ordbanken tilgjengelig, med over 1,2 millioner ord, inkludert ordenes b\'f8yning og morfologiske beskrivelse.\
\
Med en s\'e5 stor ordliste er det viktig at ordsp\'f8rringene utf\'f8res tilstrekkelig raskt. Spesielt CompoundSplitter vil gj\'f8re en stor mengde sp\'f8rringer etter ord med en \'f8vre grense p\'e5 $O(n!)$, hvor $n$ er lengden p\'e5 strengen som fors\'f8kes deles. En naiv l\'f8sning med et array vil v\'e6re uakseptabel. Det vi ser er at listen over ord vil v\'e6re statisk for hver kj\'f8ring, ingen nye ord vil legges til, vi trenger kun \'e5 gj\'f8re raske oppslag. Trie-strukturen, beskrevet i kapittel \\ref\{sec:trie\}, har sv\'e6rt rask s\'f8ketid for oppslag, med \'f8vre grense $O(m\\cdot |\\sum|)$ eller $O(m)$ med ytterligere optimalisering, dog noe p\'e5 bekostning av oppstartstid n\'e5r ordlisten skal lastes inn i trestrukturen. Men dette vil v\'e6re en god oppveing med tanke p\'e5 den store mengden sp\'f8rringer som m\'e5 gj\'f8res for hver kj\'f8ring. Jeg vil derfor implementere Dictionary som en trie-struktur.\
\
Innlastingstiden til ordlisten har blitt noe optimalisert. Ved f\'f8rste oppstart vil ordbanken leses som vanlig fra fil og satt inn i en trie-struktur. Denne objektstrukturen skrives s\'e5 direkte til fil, som gj\'f8r at ved etterf\'f8lgende oppstart av programmet leses objektstrukturen direkte inn i minnet.\
\
Hovedfunksjonene i grensesnittet til modulen er: \
\\begin\{itemize\}\
\\item Utf\'f8re sp\'f8rringer og hente ut ord fra ordlisten (om ordet ikke eksisterer returneres nil). Sp\'f8rringer etter forkortelser (markert som fork i ordlisten) vil ogs\'e5 returnere nil da forkortelser ikke skal deles. \
\\item Hente paradigmebeskrivelsen til et ord (spesielt ordklasse og b\'f8yningsendelse). \
\\item Hente (om tilfelle) prefiks eller sufiks av et ord. Dette gj\'f8res ved et oppslag i arrayer som inneholder prefikser og suffikser (beskrevet i kapittel \\ref\{sec:orddanning\}) og en sjekk om noen av disse er er prefikser eller suffikser av det angitte ordet. Videre gj\'f8res det ogs\'e5 et oppslag om resten av strengen er et gyldig ord i ordbanken. \
\\item Svare om en streng inneholder \'e9n eller fler stavelser (returnerer true eller false). Dette gj\'f8res noe naivt ved en sjekk om strengen inneholder mer enn \'e9n vokalgruppe adskilt av konsonanter, noe som gj\'f8r at ord som \'abreelt\'bb (to stavelser) blir feilaktig antatt til \'e5 ha \'e9n stavelse.\
\\end\{itemize\}\
\
\\subsubsection\{CompoundSplitter\}\
\
For \'e5 kunne dele sammensatte ord etter ordleddsregelen (se kapittel \\ref\{sec:orddeling\}) trenger vi \'e5 finne leddene som ordet best\'e5r av. Dette gj\'f8res, som beskrevet over, i to steg. CompoundSplitter gj\'f8r f\'f8rste steg steg i denne prosessen ved \'e5 finne flest mulige lovlige dekomponeringer av et ord.\
\
Dette gj\'f8res ved at strengen dekomponeres i alle mulige kombinasjoner av substrenger, hvor det s\'e5 gj\'f8res et oppslag i Dictionary-modulen om hver substreng er et gyldig ord. Hvis hver substreng-oppdeling resulterer i et gyldig ord, legges dekomponeringen til en liste over mulige l\'f8sninger. Hvis den m\'f8ter p\'e5 en \'abs\'bb eller \'abe\'bb i strengen testes det for om dette kan v\'e6re en binde-bokstav (se kapittel \\ref\{sec:ord-bind1\}). Det vil si at substreng f\'f8r bindebokstav og substreng etter bindebokstav i seg selv m\'e5 v\'e6re gyldige dekomponeringer. Hvis dette er tilfelle legges oppdelingen til listen over mulige l\'f8sninger. N\'e5r alle oppdelinger er funnet og testet returneres listen. Jeg har valgt en noe naiv rekursiv implementasjon av denne algoritmen (blant annet kunne den inkludert memoisering for \'e5 unng\'e5 fors\'f8k p\'e5 dekomponering av samme substrenger), da jeg ser p\'e5 tidsoptimalisering som en sekund\'e6r prioritert i denne oppgaven. Den rekursive funksjonen fungerer som f\'f8lger (se ogs\'e5 kodesnutt \\ref\{listing:compound-splitter\}):\
\
\pard\pardeftab720
\cf0 \\begin\{listing\}[H]\
\\inputminted[firstline=30,lastline=77,frame=single,gobble=4,linenos=true,breaklines=true]\{ruby\}\{project/hyphenator/lib/hyphenator/compound_splitter.rb\}\
\\caption\{Example from external file\}\
\\label\{listing:compound-splitter\}\
\\end\{listing\}\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural
\cf0 \
\\begin\{itemize\}\
\\item L\'f8p gjennom ordet fra venstre til h\'f8yre og sl\'e5 opp substringen i ordlisten, hvis det ikke gir treff, fortsett, ellers;\
\\item hvis vi er p\'e5 enden av stringen, da har vi en l\'f8sning som legges til en liste over mulige tolkninger og vi returnerer. Ellers;\
\\item gj\'f8r et rekursivt kall med resten av stringen (suffikset) som parameter til funksjonen.\
\\end\{itemize\}\
\
Det ble ogs\'e5 gjort et fors\'f8k p\'e5 \'e5 implementere tolkning av trippelskrevet konsonant (fotballag som delt skal st\'e5 som fotball+lag).  Det fungerer ved: n\'e5r vi m\'f8ter dobbeltskrevet konsonant hvor disse b\'e5de kan tilh\'f8re forleddet (fotball) \\textit\{og\} hvor en kan tilh\'f8re etterledd (fotbal[l]+lag), vil et en ekstra konsonant settes inn og et ekstra tolkningsalternativ genereres: \\textit\{fotball+lag\}. Dette m\'e5tte jeg g\'e5 bort fra (kommentert ut i kodesnutt), da den genererte flere feil tolkninger en den l\'f8ste problemer. Dette er samme konklusjon trukket av Sj\'f6berg og Kann \\cite\{sjobergh2004finding\}. Se kapittel \\ref\{sec:konklusjon\} for mer detaljert diskusjon.\
\pard\pardeftab720
\cf0 \
Resultatet fra CompoundSplitter returneres som et array med \'e9n eller fler dekomponerte strenger p\'e5 formen \'93eple+tre\'94 eller \'93hest+e+sal\'94 ved tilfelle av bindebokstav.\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural
\cf0 \
\\subsubsection\{CoumpoundInterpreter\}\
\
CompoundInterpreter har som oppgave \'e5 velge ut dekomponeringen som mest sannsynlig er den korrekte eller den \'f8nskede, n\'e5r det finnes fler enn \'e9n mulig dekomponering. Dette er en kompleks prosess og ikke lett \'e5 f\'e5 til helt korrekt (hvordan skal vi velge hvem som er korrekt av dekomponeringene \'93f\'f8rti+tall\'94 og \'93f\'f8r+titall\'94?). Sj\'f6berg og Kann presenterer seks forskjellige metoder for dette, inkludert en hybridl\'f8sning (se kapittel \\ref\{sec:sammensatt-analyse\}). Av tidshensyn velger jeg i f\'f8rste omgang \'e5 implementere teknikken som ser p\'e5 \\textit\{antall komponenter\} i ordet. Den har en enkel implementasjon og gir tilfredstillende med korrekte tolkninger, 90 \\%. Johannesen og Hauglin presenterer ogs\'e5 denne metoden i artikkelen \'abAn automatic analysis of Norwegian compounds\'bb\\cite\{johannessen1996automatic\}. I tillegg vil jeg benytte informasjon om tolkning av fugebokstaver fra sistnevnte artikkel. Dette beskrives n\'e6rmere i kapittel \\ref\{sec:eph\}.\
\
Hvordan velges en tolkning? N\'e5r det eksisterer mer en \'e9n mulig dekomponering av et ord vil den eller de tolkningene som har f\'e6rrest og likt antall ledd bli valgt ut. Om det eksisterer flere tolkninger med likt antall ledd (som ofte er tilfelle) vil de bli videre analysert. Om dekomponeringen inneholder en fugebokstav vil EphenthesisAnalyser analysere ordet videre (se neste kapittel). Om ikke det er tilfellet vil tolkningen med det lengste etterleddet bli valgt og returnert som et svar. Eksempelvis er \'93hus+vinduene\'94 foretrukket fremfor \'93husvin+duene\'94. \
\
Som input f\'e5r modulen et array med mulige dekomponeringer: [\'93hus+vin+duene\'94, \'93husvin+duene\'94, \'93hus+vinduene\'94 \'85] og returnerer et array med den tolkningen den anser som sannsynlig mest \'f8nsket p\'e5 formen: [hus, vinduene]. \
\
Modulen vil motta en liste med to eller flere mulige dekomponeringer av et ord i en liste (fra CompoundSplitter). De vil da komme inn som et array p\'e5 formen [\'93lese+sal+sturer\'94, \'93lese+sal+s+turer\'94]. N\'e5r en enkelt s eller enkelt e st\'e5r alene betyr det alltid at det er en bindebokstav, siden de ikke er selvstendige ord p\'e5 egenh\'e5nd og det er ingen annen tolkning en bindebokstav som er en lovlig tolkning hvor disse bokstavene st\'e5r alene. Modulen vil g\'e5 gjennom hver og en tolkning og analysere dem. \
\
\
Det f\'f8rste som sjekkes er om dekomponeringen inneholder en bindebokstav, hvis den gj\'f8r det vil den bli sendt til modulen EphenthesisAnalyser (se sekjson~\\ref\{sec:eph\}) for videre analyse. Hvis ikke dette er tilfellet g\'e5r vi videre til en analyse. I f\'f8rste omgang vil jeg kun fokusere p\'e5 \'e5 benytte meg av metoden presentert av Sj\'f6berg og Kann \\cite\{sjobergh2004finding\} som ser p\'e5 \\textit\{antall komponenter\} i ordet. Denne metoden er sv\'e6rt enkelt \'e5 implementere, men gir overraskende gode resultater (90 \\% korrekt ved tvetydig input). Hvis det er tid \'f8nsker jeg \'e5 utvikle en hybridl\'f8sning slik Sj\'f6berg og Kann ogs\'e5 repsenterer, som i tillegg til antall komponenter inkluderer informasjon om ordklasser for \'e5 kunne ytterligere gj\'f8re bedre avgj\'f8ringer.\
\
Metoden for \'e5 se p\'e5 antall komponenter fungerer som f\'f8lgende: \
\\begin\{inparaenum\}[\\itshape a\\upshape)]\
\\item velg den av analysene som har f\'e6rrest komponenter i analysen; og\
\\item ved tolkninger med likt antall komponenter, velg den med det lengste etterleddet.\
\\end\{inparaenum\}\
\\footnote\{Dette er ogs\'e5 metoden Johannesen og Hauglin \\cite\{johannessen1996automatic\} presenterer n\'e5r man skal velge mellom flere mulige tolkninger. Dog nevner de ikke punkt \\textit\{b)\}.\}. Eksempelvis \\textit\{lavast\'f8vet\} vil generere (sett at lavast\'f8v er et ord definert i ordboka):\
\
\\textit\{lava+st\'f8vet\}\\newline\
\\textit\{lavast\'f8v+et\}\\newline\
\\textit\{lava+s+t\'f8vet\}\\newline\
\\textit\{la+va+st\'f8vet\}\\newline\
\\textit\{la+vas+t\'f8vet\}\\newline\
\\textit\{lav+as+t\'f8vet\}\\newline\
\\textit\{lava+st\'f8+vet\}\\newline\
\\textit\{lava+st\'f8v+et\}\\newline\
\\textit\{lava+s+t\'f8+vet\}\\newline\
\\textit\{la+va+st\'f8v+et\}\\newline\
\\textit\{lav+as+t\'f8v+et\}\\newline\
\'85 \
\
Ved \'e5 velge alternativene med f\'e6rrest antall komponenter (\\textit\{a)\}) sitter vi igjen med to alternativer, \\textit\{lava+st\'f8vet\} og \\textit\{lavast\'f8v+et\}, og ved \'e5 videre velge alternativet med lengst etterledd (\\textit\{b)\}) sitter vi igjen med \\textit\{lava+st\'f8vet\} -- som i de fleste tilfeller vil v\'e6re den korrekte tolkningen.\
\
N\'e5r denne modulen har valgt ut den tolkning som har den h\'f8yeste sannsynligheten for \'e5 v\'e6re den korrekte, vil denne tolkningen bli sendt videre til modulen HyphenationRules, for \'e5 p\'e5f\'f8res reglenene for orddeling.\
\
\\subsubsection\{EphenthesisAnalyser\}\
\\label\{sec:eph\}\
\
I mange tilfeller vil det v\'e6re tvil om en bokstav er en bindebokstav og tilh\'f8rer forleddet, fylke\\textit\{s\}+vei, eller tilh\'f8rer etterleddet: \'f8l+\\textit\{s\}kum. Det finnes mange former for fuger i norsk spr\'e5k, totalt ni som jeg har identifisert (se kapittel \\ref\{sec:fuge-bokstav\}). Ved \'e5 generere tolkninger som tar h\'f8yde for alle disse variantene av fugebokstaver kan en risikere \'e5 overgenerere antall tolkninger som til slutt f\'f8rer til flere feiltolkninger en hva den l\'f8ser. I f\'f8lge Munthe (1972, i Johannesen og Hauglins artikkel \'abAn automatic analysis of Norwegian compounds\'bb \\cite\{johannessen1996automatic\}) er 10,4 \\% av alle ord i norske tekster sammensatte. Av disse er omtrent 75 \\% av ordene satt sammen med en nullfuge; hvor leddene er satt sammen uten en bindebokstav. Eksempelvis \'93troll+\'f8ye\'94 \\cite\{johannessen1996automatic\}. Det betyr at rundt 25% av alle sammensatte ord i norsk inneholder en bindebokstav. For \'e5 kunne ta stilling til hvilke bindebokstaver jeg vil ta h\'f8yde for i b\'e5de genereringen av tolkninger med bindebokstav (i CompoundSplitter) og i modulen for tolkning av bindebokstaver (EphenthesisAnalyser) s\'e5 jeg det \'f8nskelig \'e5 finne ut av en videre prosentvis fordeling av frekvensen til de forskjellge bindebokstavene (se neste kapittel, \\ref\{sec:fuge-frekvens\}). \
\
\\input\{content/frekvens_av_fugebokstaver.tex\}\
\
\\paragraph\{Hvordan fungerer modulen?\}\
\
Tolkning av bindebokstaver i sammensatte ord er et komplekst problem og det finnes ingen entydige regler for hvordan bindebokstavene opptrer. Men noen regler er gitt av Johannesen og Hauglin \\cite\{johannessen1996automatic\} (beskrevet i kapittel \\ref\{sec:reg-bind\}). Jeg vil st\'f8tte meg til disse reglene og implementere dem s\'e5 langt det lar seg gj\'f8re. For referanse gjengir jeg reglene her:\
\
\\begin\{enumerate\}\
	\\item Tolk sammensetningen som en kombinasjon av to rotord, uten fuge om mulig\
	\\begin\{enumerate\}\
		\\item l\'f8ve-manke\
		\\item Ikke: l\'f8v-e-manke\
	\\end\{enumerate\}\
	\\item Tolkning som binde-s er foretrukket om det er en tvetydig tolkning der s-en ogs\'e5 kan ing\'e5 som f\'f8rste bokstav i et verbalt etterledd\
	\\begin\{enumerate\}\
		\\item aluminium-s-nakke\
		\\item Ikke: aluminium-snakke\
	\\end\{enumerate\}\
	\\item Tolkning som binde-s er foretrukket fremfor nullfuge om forleddet i seg selv er et sammensatt ord\
	\\begin\{enumerate\}\
		\\item lesesal-s-turer\
		\\item Ikke: lesesal-sturer\
	\\end\{enumerate\}\
	\\item Binde-s kan aldri f\'f8lge binde-e og visa versa\
	\\begin\{enumerate\}\
		\\item hest-e-sal\
		\\item Ikke: hest-e-s-al\
	\\end\{enumerate\}\
	\\item Ved to analyser som gir likt antall medlemmer og ingen bindebokstav er involvert, velg, hvis mulig, analysen som er et substantiv\
	\\begin\{enumerate\}\
		\\item hun-dyr (S)\
		\\item Ikke: hund-yr (V)\
	\\end\{enumerate\}\
	\\item Ved to like analyser med tanke p\'e5 bindebokstav og regelen over, og en av dem har et forledd som i seg selv er et sammensatt ord, velg den\
	\\begin\{enumerate\}\
		\\item fagplan-arbeid\
		\\item Ikke: fag-planarbeid\
	\\end\{enumerate\}\
	\\item Binde-e kan kun settes sammen med en stamme som har enkelt stavelse\
	\\begin\{enumerate\}\
		\\item hest-e-ekvipasje\
		\\item tre-hest-ekvipasje\
		\\item Ikke: tre-hest-e-ekvipasje\
	\\end\{enumerate\}\
	\\item Stammer kan komme f\'f8r forleddet f\'f8r -e s\'e5 lenge de ikke danner sammensetning med forleddet\
	\\begin\{enumerate\}\
		\\item konge-hus-hest-e-ekvipasje\
		\\item Ikke: konge-hus-hest-ekvipasje\
	\\end\{enumerate\}\
	\\item Binde-s opptrer ikke etter en konsonantsekvens med sibilanter, \
	\\begin\{enumerate\}\
		\\item busk-spilling\
		\\item Ikke: busk-s-pilling\
	\\end\{enumerate\}\
	\\item med mindre forleddet er sammensatt\
	\\begin\{enumerate\}\
		\\item eneb\'e6rbusk-spilling\
		\\item eneb\'e6rbusk-s-pilling\
	\\end\{enumerate\}\
	\\item Hvis forleddet er ukjent, velg analysen med det lengste etterleddet\
	\\begin\{enumerate\}\
		\\item Ibsen-stykket\
		\\item Ikke: Ibsens-tykke\
	\\end\{enumerate\}\
\\end\{enumerate\}\
\
\\subsubsection\{HyphenationRules\}\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural
\cf2 Forklar hvordan jeg deler ordene:\
IKKE DEL:\
- Enstavelsesord (Hvordan sjekker jeg dette?)\
- Forkortelser \'e5rstall etc (Gj\'f8r oppslag)\
\
USAMMENSATTE ORD:\
- Enkonsonantregelen\
	En kons til ny linje\
	Minst en vokal per linje\
	Kan deles mellom vokaler som h\'f8rer til hver sin stavelse\
	x h\'f8rer alltid til foreg\'e5ende stavelse\
	Unntakene av konsonantsammensetninger som ikke skal deles (deles f\'f8r gruppen)\
\
B\'f8yninger:\
	Del f\'e6r endelse\
	Del med enkonsonantregelen\
	Stamme som ender p\'e5 vokal? Del f\'f8r endelse n\'e5r denne ogs\'e5 ender p\'e5 vokal\
\
Avledninger:\
	Prefiks. Del etter prefiks\
		IKKE enkons\
	Suffix:\
		Start kons? \
			Forran suff, IKKE enkons\
		Start vok?\
			Forran suffix\
			Enkonsonant\
\
Sammensetninger:\
- Helst i hovedfuge\
Bindebokstav til f\'f8rste ledd\
Enkeltord i sammensetninger etter enkonsonantregelen\cf0 \
\
Vi \'f8nsker i denne modulen \'e5 finne \\textit\{alle\} mulige delepunkter for et gitt ord. Det vil si at for eksempel ordet \\textit\{utvetydighetene\} skal returneres som \\textit\{u-t-ve-ty-dig-he-t-e-ne\}:\
\
\\textit\{utve-tydighetene\} (deler mellom sammensetningene)\\newline\
\\textit\{u-tve-tydighetene\} (deler etter prefiks)\\newline\
\\textit\{u-tve-tydig-hetene\} (deler f\'f8r suffiks)\\newline\
\\textit\{u-tve-tydig-het-ene\} (deler f\'f8r endelse)\\newline\
\\textit\{u-t-ve-ty-dig-he-t-e-ne\} (deler innad etter enkonsonantregelen)\
\
I f\'f8rste omgang vil ikke denne modulen ha noen form for prioritering av delingspunkter, siden reglene for norsk orddeling ikke spesifiserer dette. Men man kan se at dette kan v\'e6re \'f8nskelig, for eksempel i rekkef\'f8lgen ordene er delt over. \
\
Flyten i modulen er skissert i figur~\\ref\{fig:hyph-flyt\}. N\'e5r et ord kommer inn vil vi f\'f8rst sjekke om det er et estavelsesord. Hvis det er tilfellet skal ordet ikke deles og det returneres. Hvis dette ikke er tilfellet sjekker vi om det er et sammensatt ord. Hvis det er sammensatt blir ordet delt i sammensetningen. Etter dette vil b\'e5de det delte sammensatte ord og ikke-sammensatte ord sjekket for prefiks, suffiks og b\'f8yningsendelse for s\'e5 \'e5 eventuelt bli delt deretter, for til slutt \'e5 bli p\'e5f\'f8rt enkonsonantregelen og returnert.\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\b\fs36 \cf0 \\subsection\{Metode\}\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\b0\fs24 \cf2 1. Hva skal (av)gj\'f8res?\
2. Alternativer\
3. Vurdering\
4. Konklusjon/valg
\b\fs36 \cf0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\b0\fs24 \cf0 \
\\subsubsection\{Valg av tiln\'e6rming\}\
\
Tre kategorier av tiln\'e6rminger til orddelingsproblemet har blitt presentert. Ordlistebaserte, m\'f8nsterbaserte og regelbaserte. De ordlistebaserte tiln\'e6rmingene er vanskelig \'e5 realisere. For de lukkede ordklassene (grammatiske- og pro-ord) vil v\'e6re mulig \'e5 definere en utt\'f8mmende liste over alle ord som h\'f8rer til kombinert med ordenes korrekte delepunkt. Problemet oppst\'e5r ved den \'e5pne klassen av ord, de leksikalske ord (ord som refererer til noe \'abi verden\'bb)\\cite\{faarlund1997norsk\}. Substantiver, verb og adjektiver er typisk \'e5pne klasser. En komplett oppstilling av alle ord i denne klassen vil v\'e6re en umulighet, etter som nye ord hele tiden kommer til og p\'e5 grunn av den produktive ordsammensetningen i norsk spr\'e5k. \\textit\{Heissaftmaskin\} vil man ikke finne i noen ordbok eller leksikon, men er like fult et lovlig norsk ord -- hvis man har en bruk for det. En ordlistebasert tiln\'e6rming er derfor d\'f8mt til \'e5 v\'e6re utilstrekkelig for \'e5 kunne dele flest mulige ord, mest mulig korrekt. Vi trenger en mer generell tiln\'e6rming.\
\
De m\'f8nsterbaserte tiln\'e6rmingene generaliserer ordlistene ved \'e5 danne m\'f8nsterlister som p\'e5 en klok m\'e5te pr\'f8ver \'e5 fange opp gjennkjennelige trekk rundt delepunktene i ord. \\TeX\{\}-algoritmen m\'e5 sies gj\'f8r dette godt. Lars Gunnar Thoresen \\cite\{thoresen1993virtuelle\} viser at ved en relativt liten m\'f8nsterliste, 1 595 m\'f8nstere (basert p\'e5 16 247 delte ord), oppn\'e5r man relativt gode resultater med 90,5 \\% av delingspunkter funnet og kun 0,97 \\% feil delingspunkter funnet. Alikevel skulle vi sett at disse tallene var forbedret. Av natur vil en slik tiln\'e6rming alltid m\'e5tte gj\'f8re en oppveing mellom antall korrekte delingspunkter som blir funnet mot antall feilaktige delepunkter. \'d8nsker man \'e5 finne n\'e6rmere 100\\% av alle delingspunkter vil ogs\'e5 prosentandelen av feiltagelser ogs\'e5 \'f8ke p\'e5 grunn av generaliseringen som m\'e5 gj\'f8res. \
\
For den siste kategorien, regelbaserte, har jeg ikke funnet mye forskning rundt. Den blir ofte sett ned p\'e5 p\'e5 grunn av dens avhengighet til \'e5 m\'e5tte utvikles spesifikt rettet mot hvert skriftspr\'e5k [TODO: Sette inn referanse til paper som hevdet dette. Knuth/Liang?]. Men slik jeg ser det er det den tiln\'e6rmingen som har mulighet til \'e5 oppn\'e5 best mulig resultater, ved \'e5 se p\'e5 morfologien til ordene og p\'e5f\'f8re de gjeldende reglene direkte. Ved en god nok ordliste til grunne vil en kunne analysere nye ord som er dannet p\'e5 grunnlag av den produktive ordsammensetningen, for s\'e5 \'e5 p\'e5f\'f8re de gjeldene reglene kombinert med en ordanalyse av affikser, avledninger og b\'f8yninger, ved hjelp av en tilgjengelig ordbok. For norsk, bokm\'e5l og nynorsk, har vi Norsk ordbank som inneholder XXX ord fullstendig med informasjon om b\'f8yningsformer, ordklasser, eventuelt morfologisk beskrivelse og paradigmebeskrivelse. \
\
Kritikken at en slik tiln\'e6rming m\'e5 utvikles spesifikt for v\'e6rt spr\'e5k ser jeg ikke p\'e5 som sv\'e6rt problematisk. Jeg ser heller p\'e5 \'f8nsket om \'e5 utvikle generelle og altomfattende l\'f8sninger [TODO: Kilde hvor Liang eller Knuth nevner dette] er problematisk. Mye av kritikken mot \\TeX\{\}-algoritmen for orddeling [TODO: Sette inn kildene] g\'e5r mye p\'e5 nettop at den er generell (faktisk i heller st\'f8rre grad er utviklet for amerikansk-engelsk orddeling), og derfor ikke fanger opp andre regler i andre spr\'e5k med ikke-standard orddeling. Eksempelvis trippelskrevet konsonant, som skrives sammen til to ved ordsammensettning, som skal legges settes tilbake ved orddeling. [TODO: Finne andre eksempler.] Spr\'e5k med produktiv ordsammensetning av ord vil alltid skape et problem, og det vil v\'e6re n\'f8dvendig med en spr\'e5kmodul som kan gjennkjenne disse om vi skal kunne dele nye sammenatte ord korrekt. Petr Sojka\\cite\{sojka1995notes\} skriver:\
\
\\begin\{quote\}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\li720\fi-720\sl288\slmult1\pardirnatural
\ls1\ilvl0\cf0 The proper solution of this problem is a language module for every language, with the ability of creating new words by composition from others. This module is based on the morhpology of a language \'85\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural
\cf0 \\end\{quote\}\
\
En slik modul vil ogs\'e5 kunne bist\'e5 i andre spr\'e5krelaterte problemer som korrektur av feilstaving\\footnote\{Eksempelvis er OpenOffice s\'e6rdels d\'e5rlig til \'e5 gjennkjenne ordsammensetninger, og foresl\'e5r ofte feilaktig korrektur.\cf2 Ha referanse \'93Observasjoner fra fora viser \'85\'94.\cf0 \} i tekstredigeringsverkt\'f8y. Sojka foresl\'e5r at en slik modul b\'f8r etterhvert bli en del av spr\'e5kst\'f8tten i operativsystemer i fremtiden. Av dette ser jeg at en regelbasert tiln\'e6rming vil v\'e6re den mest b\'e6rekraftige l\'f8sningen, med potensiale til \'e5 f\'e5 de beste resultatene, og kan v\'e6re til nytte p\'e5 andre felter innen spr\'e5kteknologi. Jeg \'f8nsker derfor \'e5 utvikle en prototype basert p\'e5 en slik tiln\'e6rming og se hvor godt den fungerer.\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\b \cf0 \\subsubsection\{Valg av fremgangsm\'e5te\}\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\b0 \cf0 \
Etter gjennomgang av orddelingsreglene \\cite\{vinje\} er det for meg tydelig at ordleddsregelen, spesielt for sammensatte ord, byr p\'e5 de st\'f8rste utfordringene n\'e5r dette skal tolkes automatisk. Ordets morfologi m\'e5 analyseres og ordet m\'e5 deles opp i de korrekte stammene, inkludert eventuelle bindebokstaver og b\'f8yningsendelse. Ofte fremtrer det ogs\'e5 sammensatte ord som kan dekomponeres til stammekomponentene p\'e5 flere forskjellige m\'e5ter: \\textit\{lese+sal+sturer\} eller \\textit\{lese+sals+turer\}. I f\'f8rste omgang ved dekomponering av sammensatte ord \'f8nsker jeg f\'f8rst \'e5 finne alle mulige dekomponeringer, for s\'e5 velge den av alternativene som mest sannsynlig er korrekt. For \'e5 kunne gj\'f8re best mulig valg vil jeg st\'f8tte meg p\'e5 arbeidet til Sj\'f6berg\\cite\{sjobergh2004finding\}, beskrevet i kapittel~\\ref\{sec:sjoberg\}. \
\
I Sj\'f6bergs metode tar han bindebokstaver (kun binde-s), med i betrakning n\'e5r ord skal dekomponeres, men det gj\'f8res noe primitivt. I fugepunktet mellom ordkomponentene blir det fors\'f8ksvis satt inn en binde-s for \'e5 se om dette gir treff. Han tar ikke andre bindebokstaver med i betrakning og heller ingen andre forhold med n\'e5r dette fors\'f8kes [TODO: Sjekke litt mer detaljert hva som gj\'f8res]. Jeg \'f8nsker \'e5 gj\'f8re dette noe mer metodisk og analysere b\'e5de binde-s og binde-e knyttet til arbeidet gjort av Johannesen og Hauglin\\cite\{johannessen1996automatic\}. \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\b \cf0 \\subsubsection\{Valg av testmetoder\}\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\b0 \cf0 \
Underveis \'f8nsker jeg \'e5 teste de kritiske komponentene hver for seg, for s\'e5 til slutt \'e5 teste helheten. Dette for \'e5 isolere ut hvor godt modulene fungerer for seg selv for \'e5 tydligere se hvor problemer eventuelt ligger og hva som kan forbedres. Modulene som er av interesse \'e5 teste er CompoundSplitter: hvor godt finner den alle mulige dekomponeringer av et sammensatt ord? CoumpoundInterpreter: i hvor stor grad velger den riktig tolkning ved flere muligheter? Og HyphenationRules: hvor godt p\'e5f\'f8rer den reglene p\'e5 ikke-sammensatte ord? Dictionary og EphenthesisAnalyser vil ikke bli testet, da f\'f8rstnevnte kun vil fungere som et grensesnitt mot Norsk ordbank med tilh\'f8rende hjelpemetoder, og sistnevnte er en hjelpemodul til CoumpoundInterpreter og vil implisitt bli testet gjennom den modulen. Til sist \'f8nsker jeg \'e5 teste helheten av modulene i samspill.\
\
\\paragraph\{Testing av CompoundSplitter\}\
\
Denne modulen \'f8nsker jeg \'e5 teste p\'e5 tre kriterier: tidsbruk for \'e5 finne mulige dekomponeringer, antall korrekte dekomponeringer og antall falske positiver (eventuelt antall feil dekomponeringer). \
\
Tid testes ved \'e5 telle antall millisekunder det tar fra start av prosedyre (ikke medberegnet tid for \'e5 lese inn Norsk ordbank til minne) til resultatliste returneres. Dette gj\'f8res for hvert enkelt ord som skal testes for s\'e5 \'e5 beregne et gjennomsnitt for helheten. Denne prosessen kan v\'e6re tidkrevende da metoden er rekursiv og alle mulige substringer m\'e5 sjekkes opp i ordlisten om det er et gyldig ord eller ikke.\
\
Korrekte dekomponeringer og falske positiver vil testes ved en egenkomponert liste best\'e5ende av enklere sammensatte ord fra Norsk ordbank som typisk har \'e9n korrekt dekomponering (eple+kake) og vanskeligere ord med flere mulige dekomponeringer og fugebokstaver:\
\
\\textit\{lesesalsturer\} som kan dekomponeres til \\textit\{lese+sal+s+turer\} eller \\textit\{lese+sal+sturer\},\
\
eller \\textit\{kulturforskeren\} som kan dekomponeres til:\\newline\
\\textit\{kultur+forskeren\}\\newline\
\\textit\{kul+tur+forskeren\} \\newline\
\\textit\{kultur+forske+ren\}\\newline\
\\textit\{kultur+forsker+en\}\\newline\
\\textit\{kul+tur+forsker+en\}\\newline\
\\textit\{\'85\}\
\
Mange av disse eksemplene vil hentes fra Johannesen og Hauglin\\cite\{johannessen1996automatic\}. En falsk positiv i dette eksempelet vil da for eksempel v\'e6re \\textit\{ku+ltur+forskeren\}, siden \\textit\{ltur\} ikke er et gyldig ord. Da det ikke finnes noen tilgjengelige testdata som viser sammensatt ord med tilh\'f8rende mulige lovlige dekomponeringer m\'e5 dette arbeidet gj\'f8res manuelt for h\'e5nd.\
\
\\paragraph\{Testing av CoumpoundInterpreter\}\
\
For denne modulen \'f8nsker jeg \'e5 gj\'f8re to tester: \'e9n med enkle sammensatte ord med nullfuge fra Norsk ordbank, da denne formen for komposisjon er det desidert mest frekvente (75 \\% av alle sammensatte ord har nullfuge\\cite\{johannessen1996automatic\} [TODO: F\'f8lge opp kilde til orginal]). Test nummer to \'f8nsker jeg \'e5 gj\'f8re med en egenkonstruert liste over ekstra problematiske sammensatte ord som er to- eller flertydige og som eventuelt inneholder binde-s eller binde-e. Flere av disse eksemplene vil jeg hente fra Johannesen og Hauglin \\cite\{johannessen1996automatic\} da flere av disse er konstruert for \'e5 illustrere vanskelighetene ved sammensatte ord med bindebokstaver. Slik vil ogs\'e5 modulen EphenthesisAnalyser ogs\'e5 bli indirekte testet.\
\
Hva som er en korrekt tolkning av et sammensatt ord er ikke helt enkelt \'e5 gi et entydig svar p\'e5. Men et kriterie er at det b\'f8r v\'e6re mest mulig likt slik et menneske intuitivt ville tolket det. \\textit\{Lesesals+turer\} fremfor \\textit\{lesesal+sturer\} og \\textit\{dop+lager\} fremfor \\textit\{do+plager\} (tolkning som er litt mer i gr\'e5sonen). Mange av disse tolkningene vil v\'e6re kontekstavhengig (mulighet for videre arbeid \'e5 inkludere tolkning via kontekst).\
\
\\paragraph\{Testing av HyphenationRules\}\
\
Av figur X [TODO: Lag figur og sett inn] ser vi at ikke-sammensatte ord g\'e5r direkte til HyphenationRules-modulen. For \'e5 isolere ut denne modulen og teste n\'f8yaktigheten til den alene vil jeg konstruere en liste med ikke-sammensatte ord som jeg ogs\'e5 deler manuelt etter reglene for orddeling. Ordlisten (ikke delte) vil bli matet inn i modulen og fors\'f8kt delt. S\'e5 gj\'f8res den en sammenligning mellom resultat og den h\'e5nddelte listen av ord. Der vil jeg se p\'e5 prosentandelen av G (good), antall riktige delepunkter funnet, B (bad), antall falske positiver og M (missed), antall riktige delepunkter ikke funnet. \
\
\\paragraph\{Testing av hele systemet\}\
\
Helheten vil bli testet med en st\'f8rre liste over (sammensatte og ikke-sammensatte) ord. For \'e5 kunne sjekke om disse deles korrekt er vi avhengig av \'e5 ha en kontrolliste over korrekte delinger vi kan sammenligne med. Som beskrevet i kapittel~\\ref\{sec:ordlister\} er det vanskelig \'e5 f\'e5 tak i lister over norske ord med alle korrekte delepunkter. Jeg vil derfor st\'f8tte meg til listen konstruert av Lars Gunnar Thoresen \\cite\{thoresen1993virtuelle\}. Listen\\footnote\{Listen over delte er tilgjengelig som vedlegg i Lars Gunnar Thoresen sin masteroppgave: https://www.duo.uio.no/handle/10852/8875\} er konstruert av 11 249 ord valgt ut fra et representativt tekstkorpus fra skj\'f8nnlitteratur og blandet faglitteratur, allmennfaglig nyhetsstoff og faglitteratur informatikk. De mest hyppige ordene ble valgt ut gjennom frekvensanalyse. Ved \'e5 bruke samme testdata som Thoresen vil det ogs\'e5 v\'e6re mulig \'e5 sammenligne mine resultater med det han fikk gjennom patgen og \\TeX\{\}-algoritmen. Som med HyphenationRules vil jeg her se p\'e5 \'e5 prosentandelen av G (good), antall riktige delepunkter funnet, B (bad), antall falske positiver og M (missed), antall riktige delepunkter ikke funnet. \
\
\\subsubsection\{M\'e5l for testmetoder\}\
\
Hvilke resultater \'f8nsker jeg \'e5 oppn\'e5 med de forskjellige testene?\
\
\\paragraph\{CompoundSplitter\}\
\
I denne komponenten \'f8nsker jeg \'e5 finne \\textit\{alle\} lovlige dekomponeringer og vil tillate noen falske positiver. Falske positiver vil (forh\'e5pentligvis) ble luket ut i neste trinn -- CompoundInterpretationSplitter. \
\
\\paragraph\{CompoundInterpretationSplitter\}\
\
Siden jeg st\'f8tter meg p\'e5 metodene til Sj\'f6berg \\cite\{sjobergh2004finding\} b\'f8r det forventes \'e5 kunne f\'e5 like god tall som hans enkelste (men ogs\'e5 godt effektive) metode som kun ser p\'e5 \\textit\{antall komponenter i tolkningen\} (pr\'f8ver \'e5 redusere). Resultatene hans ga 90 \\% korrekte tolkninger ved analyse av sammensatte ord med flere mulige tolkninger (flertydige). Om det er tid \'f8nsker jeg \'e5 teste en hybridtiln\'e6rming slik Sj\'f6berg presenterer og vi b\'f8r da forvente minst 94 \\% for flertydige ord og minst 97 \\% for alle sammensatte ord. Jeg vil utf\'f8re en noe mer presis analyse av bindebokstaver (samt inkludere binde-e), s\'e5 kanskje kan man forvente noe bedre resultat.\
\
\\paragraph\{HyphenationRules\}\
\
Denne testen gj\'f8res kun med ikke-sammensatte ord og vil derfor v\'e6re enklere \'e5 utf\'f8re siden mye av kompleksiteten ved \'e5 p\'e5f\'f8re orddelingsreglene er nettop ved analysen av sammensatte ord. Thoresen \\cite\{thoresen1993virtuelle\} sine resultater er en naturlig sammenligning og det b\'f8r forventes noe bedre resultater en hva han presenterer, 90,5 \\% korrekte og 0,97 \\% falske positiver, da dette er en enklere oppgave.\
\
\\paragraph\{Hele systemet\}\
\
Igjen er det naturlig \'e5 sammenligne med resulatene til Thoresen. For at tiln\'e6rmingen jeg presenterer skal v\'e6re av interesse for bruk b\'f8r den l\'f8se oppgaven med \'e5 dele ord minst like godt. Det kan ogs\'e5 v\'e6re interessant \'e5 se p\'e5 forskjeller i v\'e5res falske positiver (hva deler vi forskjellig og hva deler vi likt). Jeg \'f8nsker \'e5 presentere resultater for dette ogs\'e5.\
\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\b\fs36 \cf0 \\subsection\{Resultater\}\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\b0\fs24 \cf0 Jeg \'f8nsket \'e5 f\'f8rst teste alle modulene hver for seg f\'f8r jeg s\'e5 testet hele systemet samlet. Dette for i st\'f8rre grad kunne identifisere hvor eventuelle feil oppsto og i hvilke komponenter, for s\'e5 \'e5 kunne klassifisere feilene bedre. Fremgangsm\'e5ten og \'f8nskede resultater ble beksrevet i forrige kapittel. I denne teksten vil jeg beskrive hvordan testene faktisk ble gjennomf\'f8rt for s\'e5 \'e5 vise resultatene og tolke resultatene.\
\
\pard\pardeftab720\sl400
\cf0 \\begin\{table\}[h]\
\\centering\
\\begin\{tabular\}\{|l|l|r|r|r|r|\}\
\\hline\
\\textbf\{Navn\}                                 & \\textbf\{Type\} & \\textbf\{Korrekt\} & \\textbf\{Feil\} & \\textbf\{(Av)\} & \\textbf\{\\%\} \\\\ \\hline\
\\multirow\{2\}\{*\}\{\\textbf\{CompoundSplitter\}\}    & Enkel         & 152              & 4             & 156           & 97,44      \\\\ \\cline\{2-6\} \
                                              & Kompleks      & 26               & 1             & 27            & 96,30      \\\\ \\hline\
\\multirow\{2\}\{*\}\{\\textbf\{CompoundInterpreter\}\} & Enkel         & 99               & 8             & 107           & 92,52      \\\\ \\cline\{2-6\} \
                                              & Kompleks      & 60               & 11            & 71            & 84,50      \\\\ \\hline\
\\end\{tabular\}\
\\caption\{Resultater\}\
\\label\{my-label\}\
\\end\{table\}\
\
\\begin\{table\}[h]\
\\centering\
\\begin\{tabular\}\{|l|l|r|r|r|r|r|\}\
\\hline\
\\textbf\{Navn\}             & \\textbf\{Type\} & \\textbf\{G\} & \\textbf\{B\} & \\textbf\{M\} & \\textbf\{(Av)\} & \\textbf\{\\%\} \\\\ \\hline\
\\textbf\{HyphenationRules\} & Enkel         &            &            &            &               &            \\\\ \\hline\
\\textbf\{Hyphenator\}       & Komplett      & 909        & 81         & 159        & 1068          &            \\\\ \\hline\
\\end\{tabular\}\
\\caption\{G (Good), B (Bad), M (Missed).\}\
\\label\{my-label2\}\
\\end\{table\}
\b \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\fs36 \cf0 \
\\subsubsection\{CompoundSplitter\}\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\b0\fs24 \cf0 CompoundSplitter \'f8nsket jeg \'e5 teste med to forskjellige tester. En enkel test der de sammensatte ordene har f\'e5 eller \'e9n enkel dekomonering, slik som eplekake til komponentene eple+kake. Deretter en mer kompleks test hvor ordsammensettningene har flere mulige tolkninger og det ikke er enkelt gitt hvilken som er den \'93rette\'94 tolkningen: lesesalsturer til lesesals+turer (for eksempel ikke lesesal+sturer). \
\
\\paragraph\{Enkelt test\} Listen besto av 156 ord hentet fra Wiktionary sin liste over norske sammensatte ord \\footnote\{\\url\{{\field{\*\fldinst{HYPERLINK "http://no.wiktionary.org/wiki/Kategori:Sammensatte_ord_i_bokm%C3%A5l"}}{\fldrslt http://no.wiktionary.org/wiki/Kategori:Sammensatte_ord_i_bokm\'e5l}}\}\}, Norsk ordbank \\footnote\{{\field{\*\fldinst{HYPERLINK "http://www.edd.uio.no/prosjekt/ordbanken/"}}{\fldrslt http://www.edd.uio.no/prosjekt/ordbanken/}}\} og fra Lars Gunnar Thoresen sin liste over delte ord \\footnote\{{\field{\*\fldinst{HYPERLINK "https://www.duo.uio.no/handle/10852/8875"}}{\fldrslt https://www.duo.uio.no/handle/10852/8875}}\}. Filen for testing er strukturert p\'e5 f\'f8lgende m\'e5te: f\'f8rste kolonne inneholder det sammensatte ordet i sin helhet og andre kolonne, skilt med tabular, inneholder det som anses som \'f8nsket dekomponering. Eksempelvis:\
\
\\texttt\{abortmotstander	abort+motstander\}\
\
CompoundSplitter returnerer et array av mulige dekomponeringer, og et korrekt resultat fra CompoundSplitter vil v\'e6re om \'f8nsket dekomponering (kolonne to) er inneholdt som et alternativ i resultatet fra CompoundSplitter. Denne testen ga 152 korrekte resultater og 4 feil resultater av 156, alts\'e5 korrekt i 97,44 \\% av tilfellene. De fire resultatene som ble feil er som f\'f8lgende:\
\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural
\ls2\ilvl4\cf0 Input: acappellasang | Should be: acappella+sang Got: ["acappellasang"]
\b \
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural
\ls3\ilvl4
\b0 \cf0 Input: avh\'f8r | Should be: av+h\'f8r Got: ["avh\'f8r"]\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural
\ls4\ilvl4\cf0 Input: hvitveis | Should be: hvit+veis Got: ["hvi+tv+e+is", "hvit+ve+is", "hvitveis"]\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural
\cf0 Input: vennesex | Should be: venn+e+sex Got: ["ven+ne+sex", "ven+ne+s+ex", "ven+nes+ex", "venn+es+ex", "venn+ne+sex", "venn+ne+s+ex", "venn+nes+ex", "venne+sex", "venne+s+ex", "vennes+ex"]\
\
F\'f8rste feil skyldes at Ordbanken ikke inneholder ordet acappella som eget ord, eneste oppf\'f8ringene med acappella er acappellasang og acappellakor. [Avh\'f8r-feilen m\'e5 jeg fikse]. Hvitveis kommer som en feiloppf\'f8ring da veis ikke er et eget ord og dermed ikke eksisterer i Ordbanken. S\'e5 det er kanskje feil \'e5 si at hvitveis er et sammensatt ord med komponentene hvit+veis. Vennesex oppst\'e5r som feil p\'e5 grunn av et problem i m\'e5ten bindebokstaver tolkes i CompoundSplitter. Det skal aldri v\'e6re to fugebokstaver etter hverandre \\cite\{johannessen1996automatic\} og dette skaper et problem i dette tilfellet n\'e5r e og s etterhverandre kan tolkes som fugebokstaver, det blir kun tolket som binde-s og ikke som binde-e alene. S\'e5 tre av disse feilene kan rettes, mens den f\'f8rste skyldes en mangelfull oppf\'f8ring i Ordbanken.\
\
\\paragraph\{Kompleks test\} Denne testen besto av 27 ord hovedsakelig fra teksten til Hauglin og Johannessen \\cite\{johannessen1996automatic\}, som gir eksempeler som illustrerer kompliserte tolkninger ved sammensatte ord, spesielt med binde-s og binde-e. Jeg har ogs\'e5 lagt til enkelte ord som f\'e5r trippelkonsonant ved dekomponering (fotball+lag). Denne testen ga 26 riktige og 1 feiltolkning av totalt 27, en presisjon p\'e5 96,30 \\%. Feilen som oppsto er f\'f8lgende:\
\
Input: slottsvinduene | Should be: slott+s+vinduene Got: ["slott+svin+du+ene", "slott+svin+du+e+ne", "slott+svin+due+ne", "slott+svin+duene", "slottsvin+du+ene", "slottsvin+du+e+ne", "slottsvin+due+ne", "slottsvin+duene"]\
\
Dette skyldes et problem ved tolkning av trippelkonsonant slott->slott+ts og binde-s. N\'e5r trippelkonsonant ble skurdd av fikk vi 100 \\%. [Trippelkonsonant testes i simpel, husk \'e5 skrive om].\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural
\ls5\ilvl4
\b\fs36 \cf0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural
\cf0 \\subsubsection\{CompoundInterpreter\}\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\b0\fs24 \cf0 CompoundInterpreter tar inn et array av mulige dekomponeringer av et sammensatt ord, eksempelvis ["fag+plan+ar+be+id", "fag+plan+arbeid", "fagplan+ar+be+id", "fagplan+arbeid\'94], hvor s\'e5 oppgaven er \'e5 returnere den dekomponeringen som mest sannsynlig er den korrekte tolkningen. Denne modulen testes, p\'e5 lik linje med den forrige, med to eksempellister, en med simple ord og en med komplekse ord. Listene er strukturert p\'e5 f\'f8lgende form: f\'f8rste kolonne viser hva som er \'f8nsket tolkning, mens resterende kolonner, skilt med tabulator, viser mulige lovlige dekomponeringer av ordet, som CompoundInterpreter skal pr\'f8ve \'e5 velge den \'93riktige\'94 ut fra. Eksempelvis: \
\
\\texttt\{etter+navn	etternavn	etter+navn	ett+ter+navn	ett+er+navn	et+ter+navn\}\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\b \cf0 \
\\paragraph\{Enkel test\} 
\b0 Testfilen best\'e5r av 107 ord hentet fra Wiktionary sin liste over norske sammensatte ord \\footnote\{\\url\{{\field{\*\fldinst{HYPERLINK "http://no.wiktionary.org/wiki/Kategori:Sammensatte_ord_i_bokm%C3%A5l"}}{\fldrslt http://no.wiktionary.org/wiki/Kategori:Sammensatte_ord_i_bokm\'e5l}}\}\} og fra Lars Gunnar Thoresen sin liste over delte ord \\footnote\{{\field{\*\fldinst{HYPERLINK "https://www.duo.uio.no/handle/10852/8875"}}{\fldrslt https://www.duo.uio.no/handle/10852/8875}}\}. Denne testen ga 8 feil og 99 korrekte resultater av totalt 107, en n\'f8ykatighet p\'e5 92,52 \\%. Feilene som oppsto er som f\'f8lger:\
\
Should be: brakk+vann | Got: brak+kvann\
Should be: f\'f8rti+tall | Got: f\'f8r+titall\
Should be: troll+\'f8ye | Got: troll+l\'f8ye\
Should be: full+ladet | Got: ful+ladet\
Should be: etter | Got: ett+ter\
Should be: sier | Got: si+er\
Should be: takk+for+maten+taler | Got: takk+format+en+taler\
\
Feilresultat 1 og 2 skyldes at CompoundInterpreter, ved mulige dekomoneringer med likt antall komonenter, f.eks. brakk+vann og brak+kvann, vil favorisere dekomponeringen med lengst etterledd etter anbefalninger fra Haugling og Johannessen \\cite\{johannessen1996automatic\}. Dette vil f\'f8re til enkelte feiltolkninger. Troll+l\'f8ye er resultat av overgenerering fra koden som tolker trippelkonsonanter, mens ful+ladet blir valgt siden CompoundInterpreter velger den med lengst etterledd ved likt antall komponenter, men har ikke noen m\'e5te \'e5 skille ytterligere om dette er likt ogs\'e5. Det samme gjelder for takk+for+maten+taler. Etter og sier er offer for en un\'f8dvendig dekomponering. Det er vanskelig \'e5 skille om dette er et sammensatt ord som skal dekomponeres eller om det er et enkeltord.\
\
\\paragraph\{Kompleks test\} Testfilen med komplekse tester best\'e5r av 71 ord med tilh\'f8rende dekomponeringer, hentet fra samme kilder som den enkle testen, men da de som har tolkninger som eventuelt inneholder bindebokstaver, samt eksemplene fra Johannessen og Hauglin \\cite\{johannessen1996automatic\}. Den komplekse testen resulterte i 11 feil og 60 korrekte resultater av totalt 71. Det gir en n\'f8yaktighet p\'e5 84,50 \\%. Feilene som oppsto er som f\'f8lger:\
\
1. Should be: flis+legger | Got: fli+slegger\
2. Should be: fylkes+vei | Got: fylke+svei\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural
\ls6\ilvl2\cf0 3. Should be: hus+tak | Got: hu+stak\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural
\ls7\ilvl2\cf0 4. Should be: kamp+sang | Got: kamps+ang\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural
\ls8\ilvl2\cf0 5. Should be: verdenscup+seiere | Got: verdens+cupseiere\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural
\ls9\ilvl2\cf0 6. Should be: vindus+kitt | Got: vindu+skitt\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural
\ls10\ilvl2\cf0 7. Should be: tilsyn+s+organ | Got: til+synsorgan\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural
\ls11\ilvl2\cf0 8. Should be: \'f8l+skum | Got: \'f8ls+kum\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural
\ls12\ilvl2\cf0 9. Should be: krigs+maske | Got: krig+smaske\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural
\ls13\ilvl2\cf0 10. Should be: spisestue+ur | Got: spise+stueur\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural
\ls14\ilvl2\cf0 11. Should be: eineb\'e6rbusks+pilling | Got: eineb\'e6r+busk+spilling\
\
[TODO: Forklare problemene, men kan endres hvis jeg rekker \'e5 fikse]\
Problemer: \
Svei er verb, derfor \'f8nsker det fremfor fylkes.\
Slegger verb og substantiv, ene blir overskrevet i trie\
Sang er verb og substantiv og blir overskrevet\
5. er offer for lengste etterledd\
6. Overskrevet i trie\
7. Lengste etterledd\
8. Blir overkj\'f8rt i trie\
10. Offer for lengste etterledd.
\b \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\fs36 \cf0 \
\\subsubsection\{HyphenationRules\}\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\b0\fs24 \cf0 Denne modulen har som oppgave \'e5 p\'e5f\'f8re reglene for orddeling og returnere ordet delt i alle lovlige punkter. For eksempel dagen -> da-g-en. Formatet p\'e5 input er et array med den korrekte dekomponeringen (hvis det eksisterer en dekomponering) av ordet. For eksempel [\'93l\'f8ve\'94, \'93manke\'94]. HyphenationRules blir testet separat alene med en liste over usammensatte ord, for \'e5 kunne identifisere trender i eventuelle feildelte ord som opptrer. Listen er laget p\'e5 grunnlag av ordlisten til Lars Gunnar Thoresen \\footnote\{{\field{\*\fldinst{HYPERLINK "https://www.duo.uio.no/handle/10852/8875"}}{\fldrslt https://www.duo.uio.no/handle/10852/8875}}\}. Listen besto av totalt 80 ord, hvor f\'f8rste kolonne viser ordet som skal deles og andre kolonne, skilt av tabular, viser hvordan ordet korrekt skal deles. \
\
Ved kj\'f8ring av testen fikk vi 9 feil og 71 korrekte delte ord av totalt 80, en n\'f8yaktighet p\'e5 88,75 \\%. Feilene som oppst\'e5 er som f\'f8lger:\
\
Input: visste | Should be: vis-s-te Got: viss-te\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural
\ls15\ilvl1\cf0 Input: l\'f8sning | Should be: l\'f8s-ning Got: l\'f8s-n-ing\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural
\ls16\ilvl1\cf0 Input: \'f8nsket | Should be: \'f8ns-k-et Got: \'f8n-sk-et\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural
\ls17\ilvl1\cf0 Input: synes | Should be: sy-n-es Got: syn-es\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural
\ls18\ilvl1\cf0 Input: arbeid | Should be: ar-be-id Got: ar-b-eid\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural
\ls19\ilvl1\cf0 Input: riktig | Should be: rik-t-ig Got: rik-tig\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural
\ls20\ilvl1\cf0 Input: \'e5rene | Should be: \'e5re-ne Got: \'e5-r-e-ne\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural
\ls21\ilvl1\cf0 Input: deler | Should be: de-l-er Got: d-e-ler\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural
\ls22\ilvl1\cf0 Input: prosesser | Should be: pro-ses-s-er Got: pro-s-ess-er
\b \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\fs36 \cf0 \
\\subsubsection\{Helheten -- Hyphenator\}\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\b0\fs24 \cf0 Snakk om antall delinger funnet og gale\
Snakk om eksekveringstid.}